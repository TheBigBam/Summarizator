{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BART_BASE_FT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26ee174fe0d148e6bfcc62746dabdbb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56bdb1449a9b4469bc286ac1f87c5063",
              "IPY_MODEL_50b0bbea541f4af2ab2c1c6f83600919",
              "IPY_MODEL_b45646ab08ef465d9a93b45bb7ef8a39"
            ],
            "layout": "IPY_MODEL_cec1395c7e544a08848a3b7264d1034f"
          }
        },
        "56bdb1449a9b4469bc286ac1f87c5063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce28c78a8464a8fa16e2d85aaea35fb",
            "placeholder": "​",
            "style": "IPY_MODEL_4aef9d0782f74d2b8de1a0e0cc51858f",
            "value": "100%"
          }
        },
        "50b0bbea541f4af2ab2c1c6f83600919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0d6945a11f482988263bf658f33ef1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8556e0081c75427fa5485734457228bc",
            "value": 1
          }
        },
        "b45646ab08ef465d9a93b45bb7ef8a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6774ca574104bd18bae4c111e7d62a9",
            "placeholder": "​",
            "style": "IPY_MODEL_03903732c39f4942947c60cb46c46f44",
            "value": " 1/1 [00:01&lt;00:00,  1.72s/ba]"
          }
        },
        "cec1395c7e544a08848a3b7264d1034f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce28c78a8464a8fa16e2d85aaea35fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aef9d0782f74d2b8de1a0e0cc51858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d0d6945a11f482988263bf658f33ef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8556e0081c75427fa5485734457228bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6774ca574104bd18bae4c111e7d62a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03903732c39f4942947c60cb46c46f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d908c37f324fb0860bf52c9ea07a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e0a03a1f182463cb438d35c27e0ec0c",
              "IPY_MODEL_c03cd78505a4411c84e8e227180106d8",
              "IPY_MODEL_7009e79ec3a947738b5b29c84cc81817"
            ],
            "layout": "IPY_MODEL_22458da1c00a4d6385fec5a3ea4fb847"
          }
        },
        "6e0a03a1f182463cb438d35c27e0ec0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d937aec14d49fdab5bc97ab15ce9db",
            "placeholder": "​",
            "style": "IPY_MODEL_b189bdde2c3f43baa8a4e89b93ddbdf1",
            "value": "100%"
          }
        },
        "c03cd78505a4411c84e8e227180106d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4185a128f5834d2086fb8a578efdc97e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_937c8ee1ac9841729958ff17dd87b606",
            "value": 1
          }
        },
        "7009e79ec3a947738b5b29c84cc81817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_518c1d5c64294c089a8c4d2ec06e8421",
            "placeholder": "​",
            "style": "IPY_MODEL_320e3cf4a47642179e6fd64b8213287d",
            "value": " 1/1 [00:00&lt;00:00,  2.05ba/s]"
          }
        },
        "22458da1c00a4d6385fec5a3ea4fb847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d937aec14d49fdab5bc97ab15ce9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b189bdde2c3f43baa8a4e89b93ddbdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4185a128f5834d2086fb8a578efdc97e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937c8ee1ac9841729958ff17dd87b606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518c1d5c64294c089a8c4d2ec06e8421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320e3cf4a47642179e6fd64b8213287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6211e82c7a364680aaa173b1f4ced06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5975cce8e25b444eb3d74f22bc41caff",
              "IPY_MODEL_233581d5b13140cbb46709818a4fb7aa"
            ],
            "layout": "IPY_MODEL_a45ac1436f4443a2aabd96b748099c5c"
          }
        },
        "5975cce8e25b444eb3d74f22bc41caff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_227325b55a214ea59648dc689ee0e1f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f6ba0f49aad74844b45093893c96e7bc",
            "value": " 0.57MB of 0.57MB uploaded (0.00MB deduped)\r"
          }
        },
        "233581d5b13140cbb46709818a4fb7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f1afe390544315a92595ece2554aae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d71aba0572745f3b29e59f9e361ac8e",
            "value": 1
          }
        },
        "a45ac1436f4443a2aabd96b748099c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227325b55a214ea59648dc689ee0e1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ba0f49aad74844b45093893c96e7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18f1afe390544315a92595ece2554aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d71aba0572745f3b29e59f9e361ac8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8VR8kd4ipPN"
      },
      "source": [
        "\n",
        "# Fine-tuning BART Summarization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zImELPsbXADJ",
        "outputId": "f5e80f43-6546-4221-a3bb-c9e4110f611e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 23 13:50:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   47C    P8     7W /  N/A |    453MiB /  8192MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1640    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A     10948    C+G   ...g6ke6\\HPDisplayCenter.exe    N/A      |\n",
            "|    0   N/A  N/A     14780    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A     16008    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
            "|    0   N/A  N/A     25292    C+G   ...an\\app-7.36.6\\Postman.exe    N/A      |\n",
            "|    0   N/A  N/A     36284    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUn2OqI9oPQb"
      },
      "source": [
        "## Setup\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkzypz9I1O6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d120146-0c26-4c15-88e8-df853d49dc2c"
      },
      "source": [
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (7.6.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (7.28.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
            "Requirement already satisfied: jupyter-client<8.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.4.1)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
            "Requirement already satisfied: backcall in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: colorama in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
            "Requirement already satisfied: decorator in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (58.2.0)\n",
            "Requirement already satisfied: pickleshare in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.21)\n",
            "Requirement already satisfied: pygments in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
            "Requirement already satisfied: entrypoints in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: pyzmq>=13 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: pywin32>=1.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (301)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.1.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
            "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.5)\n",
            "Requirement already satisfied: argon2-cffi in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.1.0)\n",
            "Requirement already satisfied: nbconvert in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: prometheus-client in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
            "Requirement already satisfied: Send2Trash>=1.5.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: pywinpty>=1.1.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.4)\n",
            "Requirement already satisfied: cffi>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
            "Requirement already satisfied: pycparser in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: bleach in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.4)\n",
            "Requirement already satisfied: testpath in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: packaging in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\n",
            "Requirement already satisfied: webencodings in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "      - Validating: ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gaaojSBoQ5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d99803-2b1b-4fd8-dd0d-c1bc1cd74170"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install sentencepiece\n",
        "! pip install rouge_score\n",
        "! pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (4.11.3)\n",
            "Requirement already satisfied: requests in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (1.21.2)\n",
            "Requirement already satisfied: sacremoses in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (2021.10.23)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from transformers) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.2)\n",
            "Requirement already satisfied: colorama in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests->transformers) (3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests->transformers) (2.0.0)\n",
            "Requirement already satisfied: six in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: joblib in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: datasets in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: pandas in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (1.3.4)\n",
            "Requirement already satisfied: dill in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: packaging in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (1.21.2)\n",
            "Requirement already satisfied: multiprocess in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (6.0.0)\n",
            "Requirement already satisfied: xxhash in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (2.26.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from datasets) (2021.10.1)\n",
            "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from packaging->datasets) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
            "Requirement already satisfied: colorama in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from aiohttp->datasets) (4.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (0.1.96)\n",
            "Requirement already satisfied: rouge_score in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (0.0.4)\n",
            "Requirement already satisfied: absl-py in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from rouge_score) (0.15.0)\n",
            "Requirement already satisfied: nltk in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from rouge_score) (3.6.5)\n",
            "Requirement already satisfied: numpy in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from rouge_score) (1.21.2)\n",
            "Requirement already satisfied: six>=1.14.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: joblib in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nltk->rouge_score) (1.1.0)\n",
            "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nltk->rouge_score) (4.62.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nltk->rouge_score) (2021.10.23)\n",
            "Requirement already satisfied: click in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from nltk->rouge_score) (8.0.3)\n",
            "Requirement already satisfied: colorama in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from click->nltk->rouge_score) (0.4.4)\n",
            "Requirement already satisfied: wandb in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (0.12.6)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (1.4.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: PyYAML in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (8.0.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (2.26.0)\n",
            "Requirement already satisfied: six>=1.13.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (5.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: pathtools in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from wandb) (3.19.0)\n",
            "Requirement already satisfied: colorama in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.0)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in d:\\anaconda\\envs\\zrenv\\lib\\site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rimUDCQGoTAJ"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "from tabulate import tabulate\n",
        "import nltk\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zpflBQbzrvC"
      },
      "source": [
        "WANDB_INTEGRATION = True\n",
        "if WANDB_INTEGRATION:\n",
        "    import wandb\n",
        "\n",
        "    wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-q_O-hoe3g"
      },
      "source": [
        "## Model and tokenizer\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb21WY-9mavn"
      },
      "source": [
        "Hiperparámetros: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vMhyyIPobyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95f1423-2bf4-40fc-8c85-5f3818121304"
      },
      "source": [
        "#Llamado del modelo\n",
        "model_name = \"facebook/bart-base\"\n",
        "\n",
        "#Definición de modelo y tokenizador\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Se fijan los parámetros del modelo\n",
        "model.config.activation_dropout = 0.0\n",
        "model.config.classif_dropout = 0.0\n",
        "model.config.max_encoder_position_embeddings = 1024\n",
        "model.config.max_decoder_position_embeddings = 16384\n",
        "print(model.config)\n",
        "\n",
        "  # \"classif_dropout\": 0.0,\n",
        "  # \"classifier_dropout\": 0.0,\n",
        "  # \"d_model\": 768,\n",
        "  # \"decoder_attention_heads\": 12,\n",
        "  # \"decoder_ffn_dim\": 3072,\n",
        "  # \"decoder_layerdrop\": 0.0,\n",
        "  # \"decoder_layers\": 6,\n",
        "  # \"decoder_start_token_id\": 2,\n",
        "  # \"dropout\": 0.1,\n",
        "  # \"encoder_attention_heads\": 12,\n",
        "  # \"encoder_ffn_dim\": 3072,\n",
        "  # \"encoder_layerdrop\": 0.0,\n",
        "  # \"encoder_layers\": 6,\n",
        "  # \"eos_token_id\": 2,\n",
        "\n",
        "# tokenización\n",
        "encoder_max_length = 256 \n",
        "decoder_max_length = 64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_decoder_position_embeddings\": 16384,\n",
            "  \"max_encoder_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwtSPRJgomBS"
      },
      "source": [
        "## Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZfrIK8fW9DU"
      },
      "source": [
        "### Descarga y Preparación de los Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4MZ_LiNwI_T"
      },
      "source": [
        "### Cargado de Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0TPG-bEk-uU",
        "outputId": "4334cc53-8616-4cc3-f58e-5bdcff0b8f4a"
      },
      "source": [
        "train_data_txt = datasets.load_dataset(\"cnn_dailymail\", '3.0.0', split=\"train[:500]\")\n",
        "validation_data_txt = datasets.load_dataset(\"cnn_dailymail\", '3.0.0', split=\"validation[:500]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset cnn_dailymail (C:\\Users\\JumpNShootMan\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
            "Reusing dataset cnn_dailymail (C:\\Users\\JumpNShootMan\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbe750YpMfD"
      },
      "source": [
        "**Preprocess and tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyksYNwxA4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "26ee174fe0d148e6bfcc62746dabdbb2",
            "56bdb1449a9b4469bc286ac1f87c5063",
            "50b0bbea541f4af2ab2c1c6f83600919",
            "b45646ab08ef465d9a93b45bb7ef8a39",
            "cec1395c7e544a08848a3b7264d1034f",
            "6ce28c78a8464a8fa16e2d85aaea35fb",
            "4aef9d0782f74d2b8de1a0e0cc51858f",
            "7d0d6945a11f482988263bf658f33ef1",
            "8556e0081c75427fa5485734457228bc",
            "e6774ca574104bd18bae4c111e7d62a9",
            "03903732c39f4942947c60cb46c46f44",
            "17d908c37f324fb0860bf52c9ea07a3e",
            "6e0a03a1f182463cb438d35c27e0ec0c",
            "c03cd78505a4411c84e8e227180106d8",
            "7009e79ec3a947738b5b29c84cc81817",
            "22458da1c00a4d6385fec5a3ea4fb847",
            "e5d937aec14d49fdab5bc97ab15ce9db",
            "b189bdde2c3f43baa8a4e89b93ddbdf1",
            "4185a128f5834d2086fb8a578efdc97e",
            "937c8ee1ac9841729958ff17dd87b606",
            "518c1d5c64294c089a8c4d2ec06e8421",
            "320e3cf4a47642179e6fd64b8213287d"
          ]
        },
        "outputId": "1d43f935-b392-4e3f-b712-b1f7c14941fe"
      },
      "source": [
        "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
        "    source, target = batch[\"article\"], batch[\"highlights\"]\n",
        "    source_tokenized = tokenizer(\n",
        "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
        "    )\n",
        "    target_tokenized = tokenizer(\n",
        "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
        "    )\n",
        "\n",
        "    batch = {k: v for k, v in source_tokenized.items()}\n",
        "    # Ignore padding in the loss\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
        "        for l in target_tokenized[\"input_ids\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "\n",
        "train_data = train_data_txt.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=train_data_txt.column_names,\n",
        ")\n",
        "\n",
        "validation_data = validation_data_txt.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=validation_data_txt.column_names,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ee174fe0d148e6bfcc62746dabdbb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17d908c37f324fb0860bf52c9ea07a3e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ViBmMopWfb"
      },
      "source": [
        "## Training\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EfTztMPv2vG"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNCGl2sYl2p"
      },
      "source": [
        "# Borrowed from https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "metric = datasets.load_metric(\"rouge\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract a few results from ROUGE\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
        "    ]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O1EeUi-pbPA"
      },
      "source": [
        "### Training arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R9d7ELIpX9F"
      },
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    num_train_epochs=5,  # Bajo por ahora\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=3,  # \n",
        "    per_device_eval_batch_size=3,\n",
        "    learning_rate=3e-04, #3e-05\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    label_smoothing_factor=0.1,\n",
        "    predict_with_generate=True, #Para métricas ROUGE\n",
        "    logging_dir=\"logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=validation_data,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzcsz3gKplPO"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpg2a0mfoD-l"
      },
      "source": [
        "Wandb integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns6c0cMuWDXp",
        "outputId": "872c9f6d-8dd5-48d0-ce7d-35f1d8858d28"
      },
      "source": [
        "wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'wandb' from 'D:\\\\Anaconda\\\\envs\\\\ZREnv\\\\lib\\\\site-packages\\\\wandb\\\\__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdaVPp9doF1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9c0b0dfd-c351-4c7a-db82-c1477b708fcc"
      },
      "source": [
        "if WANDB_INTEGRATION:\n",
        "    wandb_run = wandb.init(\n",
        "        project=\"BART_BASE_FT\",\n",
        "        config={\n",
        "            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
        "            \"learning_rate\": training_args.learning_rate,\n",
        "            \"dataset\": \"cnn_dailymail\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H%M%S\")\n",
        "    wandb_run.name = \"run_\" + current_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/zeroranger/BART_BASE_FT/runs/25fnuqj4\" target=\"_blank\">fancy-sun-1</a></strong> to <a href=\"https://wandb.ai/zeroranger/BART_BASE_FT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEtd_a7TPpkd"
      },
      "source": [
        "Evaluate before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yveDiz7pm3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "6cf957b7-c1d0-4beb-a6e2-5ffe715b106f"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [167/167 01:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 5.8895792961120605,\n",
              " 'eval_rouge1': 18.2709,\n",
              " 'eval_rouge2': 5.4735,\n",
              " 'eval_rougeL': 14.329,\n",
              " 'eval_rougeLsum': 16.258,\n",
              " 'eval_gen_len': 20.0,\n",
              " 'eval_runtime': 167.5795,\n",
              " 'eval_samples_per_second': 2.984,\n",
              " 'eval_steps_per_second': 0.997}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkRb7hvgPrf2"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZLM53u6mXyF"
      },
      "source": [
        "torch.cuda.is_available()\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgJsTMNan2cQ",
        "outputId": "f1af87ef-e972-462f-8426-3ab8d1cc8577"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYcYcbkr7ZZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "outputId": "e1106dc5-fd4d-4096-b323-c584c5e6b214"
      },
      "source": [
        "#%%wandb\n",
        "# uncomment to display Wandb charts\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 3\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 835\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='835' max='835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [835/835 03:44, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.210500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.184600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.956200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.773700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.872500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.764400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.052300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.008000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.982200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.280700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to results\\checkpoint-500\n",
            "Configuration saved in results\\checkpoint-500\\config.json\n",
            "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
            "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
            "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=835, training_loss=3.440223894861644, metrics={'train_runtime': 234.1208, 'train_samples_per_second': 10.678, 'train_steps_per_second': 3.567, 'total_flos': 476356608000000.0, 'train_loss': 3.440223894861644, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3C-4SfOPssY"
      },
      "source": [
        "Evaluate after fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-QyUtCRH9DO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "e16f4e6e-7a48-4a9e-f4fe-5924b1670b5c"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='334' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [167/167 08:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.3277082443237305,\n",
              " 'eval_rouge1': 23.9348,\n",
              " 'eval_rouge2': 9.2392,\n",
              " 'eval_rougeL': 19.2516,\n",
              " 'eval_rougeLsum': 21.5988,\n",
              " 'eval_gen_len': 20.0,\n",
              " 'eval_runtime': 86.9753,\n",
              " 'eval_samples_per_second': 5.749,\n",
              " 'eval_steps_per_second': 1.92,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClRTrG2ETUm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576,
          "referenced_widgets": [
            "6211e82c7a364680aaa173b1f4ced06d",
            "5975cce8e25b444eb3d74f22bc41caff",
            "233581d5b13140cbb46709818a4fb7aa",
            "a45ac1436f4443a2aabd96b748099c5c",
            "227325b55a214ea59648dc689ee0e1f0",
            "f6ba0f49aad74844b45093893c96e7bc",
            "18f1afe390544315a92595ece2554aae",
            "1d71aba0572745f3b29e59f9e361ac8e"
          ]
        },
        "outputId": "963fc295-1c88-49d5-ead6-55804f5bf4e2"
      },
      "source": [
        "if WANDB_INTEGRATION:\n",
        "    wandb_run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 32472... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6211e82c7a364680aaa173b1f4ced06d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>▁▁</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/rouge1</td><td>▁█</td></tr><tr><td>eval/rouge2</td><td>▁█</td></tr><tr><td>eval/rougeL</td><td>▁█</td></tr><tr><td>eval/rougeLsum</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▅▅▄▄▄▃▃▃▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>20.0</td></tr><tr><td>eval/loss</td><td>4.32771</td></tr><tr><td>eval/rouge1</td><td>23.9348</td></tr><tr><td>eval/rouge2</td><td>9.2392</td></tr><tr><td>eval/rougeL</td><td>19.2516</td></tr><tr><td>eval/rougeLsum</td><td>21.5988</td></tr><tr><td>eval/runtime</td><td>86.9753</td></tr><tr><td>eval/samples_per_second</td><td>5.749</td></tr><tr><td>eval/steps_per_second</td><td>1.92</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>835</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>2.2807</td></tr><tr><td>train/total_flos</td><td>476356608000000.0</td></tr><tr><td>train/train_loss</td><td>3.44022</td></tr><tr><td>train/train_runtime</td><td>234.1208</td></tr><tr><td>train/train_samples_per_second</td><td>10.678</td></tr><tr><td>train/train_steps_per_second</td><td>3.567</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">fancy-sun-1</strong>: <a href=\"https://wandb.ai/zeroranger/BART_BASE_FT/runs/25fnuqj4\" target=\"_blank\">https://wandb.ai/zeroranger/BART_BASE_FT/runs/25fnuqj4</a><br/>\n",
              "Find logs at: <code>.\\wandb\\run-20211123_143254-25fnuqj4\\logs</code><br/>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gSLVnGL9bol"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDwj24cfILS6"
      },
      "source": [
        "**Generate summaries from the fine-tuned model and compare them with those generated from the original, pre-trained one.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV64-XdA_rOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3cfc4e-6c51-4d61-ce71-72176a6938c6"
      },
      "source": [
        "def generate_summary(test_samples, model):\n",
        "    inputs = tokenizer(\n",
        "        test_samples[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=encoder_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return outputs, output_str\n",
        "\n",
        "\n",
        "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "test_samples = validation_data_txt.select(range(16))\n",
        "\n",
        "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
        "summaries_after_tuning = generate_summary(test_samples, model)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at C:\\Users\\JumpNShootMan/.cache\\huggingface\\transformers\\f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.da0f3c0e2dc1c2fecc46738a1ebf4806f2fc36aae3d5c1947f21e063e7cab34b\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\JumpNShootMan/.cache\\huggingface\\transformers\\486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7IPtJLjCcmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfd53f5-c93c-4936-c7be-4ffbfcc7e49f"
      },
      "source": [
        "print(\n",
        "    tabulate(\n",
        "        zip(\n",
        "            range(len(summaries_after_tuning)),\n",
        "            summaries_after_tuning,\n",
        "            summaries_before_tuning,\n",
        "        ),\n",
        "        headers=[\"Id\", \"Summary after\", \"Summary before\"],\n",
        "    )\n",
        ")\n",
        "print(\"\\nTarget summaries:\\n\")\n",
        "print(\n",
        "    tabulate(list(enumerate(test_samples[\"highlights\"])), headers=[\"Id\", \"Target summary\"])\n",
        ")\n",
        "print(\"\\nSource documents:\\n\")\n",
        "print(tabulate(list(enumerate(test_samples[\"article\"])), headers=[\"Id\", \"Document\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Id  Summary after                                                                                              Summary before\n",
            "----  ---------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------\n",
            "   0  NEW: Crosby was driving at approximately 50 mph when he struck a jogger.                                   (CNN)Singer-songwriter David Crosby hit a jogger with his car\n",
            "   1  Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members                       (CNN)Sigma Alpha Epsilon is under fire for a video showing party-\n",
            "   2  Candida Moss is an adviser on the \"True Cross\" episode.                                                    (CNN)I'm Candida Moss and I am professor of New Testament and Early\n",
            "      The \"\n",
            "   3  Ferguson Police Chief Thomas Jackson announces his resignation after a damning Justice Department report.  (CNN)Ferguson is crumbling. The cowardly and reprehensible shooting Wednesday night of\n",
            "   4  Bill Clinton defends his family foundation's practice of taking money from foreign countries.              Coral Gables, Florida (CNN)Former President Bill Clinton on Saturday defended his\n",
            "      Clinton\n",
            "   5  Meerkat launched February 27 and has been adding users rapidly ever since.                                 (CNN)If you haven't yet been asked by friends or co-workers to\n",
            "      The\n",
            "   6  John Gordon Mein, U.S. ambassador to Guatemala, was assassinated in August                                 (CNN)Diplomacy can be dangerous. U.S. diplomats have come\n",
            "   7  Debra Milke is the second woman exonerated from death row in the United States                             (CNN)Debra Milke spent 22 years on death row, convicted of conspiring\n",
            "   8  A British health care worker in Sierra Leone has tested positive for Ebola, a UK health                    (CNN)A British military health care worker in Sierra Leone has tested positive for Ebola\n",
            "   9  Iraqi forces appear to be making progress on a major offensive against ISIS.                               Baghdad, Iraq (CNN)Iraqi forces appeared Wednesday to be making progress\n",
            "      The\n",
            "  10  Jesse Kasdorf: Conflicts between faith and religion can be both exhilarating                               (CNN)The gift of an inquiring mind can be both exhilarating and tort\n",
            "  11  This page includes the show Transcript.                                                                    March 3, 2015. You may not be familiar with the city of Tikrit,\n",
            "      Use the Transcript to help students with reading comprehension\n",
            "  12  Jonathan acknowledges the great opportunity he was given to lead this country.                             (CNN)Incumbent Goodluck Jonathan phoned former military leader Muhammadu B\n",
            "      The 72-\n",
            "  13  There has been an unusually high number of sea lion \"strandings\" since January                             (CNN)Wildlife services in California are being pushed to their limits this year.\n",
            "  14  Investigators found a letter in the waste bin of his Dusseldorf apartment.                                 (CNN)Germanwings co-pilot Andreas Lubitz was hiding an illness from\n",
            "  15  Julne Moore wins the Oscar for \"Still Alice\"                                                               (CNN)I was deeply moved watching Julianne Moore win the Oscar for \"Still\n",
            "      The Alzheimer's disease has\n",
            "\n",
            "Target summaries:\n",
            "\n",
            "  Id  Target summary\n",
            "----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "   0  Accident happens in Santa Ynez, California, near where Crosby lives .\n",
            "      The jogger suffered multiple fractures; his injuries are not believed to be life-threatening .\n",
            "   1  Sigma Alpha Epsilon is being tossed out by the University of Oklahoma .\n",
            "      It's also run afoul of officials at Yale, Stanford and Johns Hopkins in recent months .\n",
            "   2  Religion professor Candida Moss appears in each episode of the program .\n",
            "      Moss was part of the original study to determine if relics found in Bulgaria could be the bones of John the Baptist.\n",
            "   3  Two police officers were shot Wednesday in Ferguson .\n",
            "      Hank Johnson, Michael Shank: Policing style needs rethink .\n",
            "   4  Clinton Foundation has taken money from foreign governments .\n",
            "      Bill Clinton:  \"I believe we have done a lot more good than harm\"\n",
            "   5  Join Meerkat founder Ben Rubin for a live chat at 2 p.m. ET Wednesday .\n",
            "      Follow @benrbn and @lauriesegallcnn on Meerkat .\n",
            "      Use hashtag #CNNInstantStartups to join the conversation on Twitter .\n",
            "   6  Several U.S. diplomats have died after being attacked .\n",
            "      They include then-Ambassadors Christopher Stevens, John Mein and Francis Meloy .\n",
            "   7  Debra Milke was convicted of murder in her son's death, given the death penalty .\n",
            "      There was no evidence tying her to the crime, but a detective said she confessed .\n",
            "      This detective had a \"history of misconduct,\" including lying under oath .\n",
            "   8  Spokesperson: Experts are investigating how the UK military health care worker got Ebola .\n",
            "      It is being decided if the military worker infected in Sierra Leone will return to England .\n",
            "      There have been some 24,000 reported cases and 10,000 deaths in the latest Ebola outbreak .\n",
            "   9  Iraqi forces make some progress as they seek to advance toward Tikrit .\n",
            "      The city, best known to Westerners as Saddam Hussein's birthplace, was taken by ISIS in June .\n",
            "  10  Kyra Phillips became a born-again Christian as a teen .\n",
            "      She attended a Christian college, but left after her sophomore year .\n",
            "      Phillips says she now considers herself a seeker of spiritual enlightenment .\n",
            "  11  This page includes the show Transcript .\n",
            "      Use the Transcript to help students with reading comprehension and vocabulary .\n",
            "      At the bottom of the page, comment for a chance to be mentioned on CNN Student News.  You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "  12  Incumbent President Goodluck Jonathan acknowledges defeat, says he delivered on promise of fair elections .\n",
            "      Muhammadu Buhari's party says Jonathan called to concede even before final results are announced .\n",
            "      Buhari is a 72-year-old retired major general who ruled in Nigeria in the 1980s .\n",
            "  13  \"There has been an unusually high number of sea lions stranded since January,\" NOAA representative says .\n",
            "      The speculation is mothers are having difficulty finding food, leaving pups alone too long or malnourished .\n",
            "  14  Reuters reports German newspaper says Lubitz took break in 2009 due to depression .\n",
            "      Ripped medical-leave notes found at his home indicate co-pilot hid an illness, officials say .\n",
            "      Investigators found no goodbye letter or evidence of political or religious motivation .\n",
            "  15  Maria Shriver's father was stricken by Alzheimer's, a growing scourge in U.S.\n",
            "      Women are disproportionately affected as sufferers and caregivers, she says .\n",
            "      Wipe Out Alzheimer's Challenge is launching to fill in for lagging government funding, she says .\n",
            "\n",
            "Source documents:\n",
            "\n",
            "  Id  Document\n",
            "----  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "   0  (CNN)Singer-songwriter David Crosby hit a jogger with his car Sunday evening, a spokesman said. The accident happened in Santa Ynez, California, near where Crosby lives. Crosby was driving at approximately 50 mph when he struck the jogger, according to California Highway Patrol Spokesman Don Clotworthy. The posted speed limit was 55. The jogger suffered multiple fractures, and was airlifted to a hospital in Santa Barbara, Clotworthy said. His injuries are not believed to be life threatening. \"Mr. Crosby was cooperative with authorities and he was not impaired or intoxicated in any way. Mr. Crosby did not see the jogger because of the sun,\" said Clotworthy. According to the spokesman, the jogger and Crosby were on the same side of the road. Pedestrians are supposed to be on the left side of the road walking toward traffic, Clotworthy said. Joggers are considered pedestrians. Crosby is known for weaving multilayered harmonies over sweet melodies. He belongs to the celebrated rock group Crosby, Stills & Nash. \"David Crosby is obviously very upset that he accidentally hit anyone. And, based off of initial reports, he is relieved that the injuries to the gentleman were not life threatening,\" said Michael Jensen, a Crosby spokesman. \"He wishes the jogger a very speedy recovery.\"\n",
            "   1  (CNN)Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members singing a racist chant. SAE's national chapter suspended the students, but University of Oklahoma President David Boren took it a step further, saying the university's affiliation with the fraternity is permanently done. The news is shocking, but it's not the first time SAE has faced controversy. SAE was founded March 9, 1856, at the University of Alabama, five years before the American Civil War, according to the fraternity website. When the war began, the group had fewer than 400 members, of which \"369 went to war for the Confederate States and seven for the Union Army,\" the website says. The fraternity now boasts more than 200,000 living alumni, along with about 15,000 undergraduates populating 219 chapters and 20 \"colonies\" seeking full membership at universities. SAE has had to work hard to change recently after a string of member deaths, many blamed on the hazing of new recruits, SAE national President Bradley Cohen wrote in a message on the fraternity's website. The fraternity's website lists more than 130 chapters cited or suspended for \"health and safety incidents\" since 2010. At least 30 of the incidents involved hazing, and dozens more involved alcohol. However, the list is missing numerous incidents from recent months. Among them, according to various media outlets: Yale University banned the SAEs from campus activities last month after members allegedly tried to interfere with a sexual misconduct investigation connected to an initiation rite. Stanford University in December suspended SAE housing privileges after finding sorority members attending a fraternity function were subjected to graphic sexual content. And Johns Hopkins University in November suspended the fraternity for underage drinking. \"The media has labeled us as the 'nation's deadliest fraternity,' \" Cohen said. In 2011, for example, a student died while being coerced into excessive alcohol consumption, according to a lawsuit. SAE's previous insurer dumped the fraternity. \"As a result, we are paying Lloyd's of London the highest insurance rates in the Greek-letter world,\" Cohen said. Universities have turned down SAE's attempts to open new chapters, and the fraternity had to close 12 in 18 months over hazing incidents.\n",
            "   2  (CNN)I'm Candida Moss and I am professor of New Testament and Early Christianity at the University of Notre Dame. I was an adviser on the \"True Cross\" episode and served as one of the many on-camera experts in CNN's \"Finding Jesus\" series, which currently airs on Sundays. Viewers were invited to tweet and post their questions on the \"Finding Jesus\" Facebook page during the show. Below are some of the more interesting questions and my answers to them. My apologies to everyone I didn't get to. Feel free to tweet your questions to me directly. Herb Scribner: Can anyone explain to me what the Bible's deal is with 40 days/nights? Moss: It's more an interest in the number 40. In the Hebrew Bible the people of Israel wander in the wilderness for 40 years before they reach the Holy Land. The flood lasts for 40 days and nights; Moses spends 40 days and nights on the mountain; Goliath spends 40 days encouraging the Israelites to challenge him before David steps up; 40 is a common age for people to be when they get married; in the book of Judges it is always 40 years between judges; and David and Solomon each reigned for 40 years. What we can take away from all of this is that people in the ancient world saw 40 as suggesting a full, complete period of time. It's sort of like a narrative stock number, in the way that modern jokes follow the rule of three. Yalanda M. Price: Was there any division between the followers of Jesus and the followers of John the Baptist? Moss: One of the interesting things about the relationship between Jesus and John the Baptist is that John doesn't lay down his tools and follow Jesus after he baptizes him. Nor, it seems, did John's disciples. They had separate ministries and, while there may have been contact between the two groups, they were also de facto competitors in the ancient religious marketplace. There are some hints in the New Testament that Jesus and his followers had to differentiate themselves from John by stating that Jesus' baptism was better (Acts 11:6) and countering the idea that Jesus was actually John raised from the dead (Matthew 14:2) Some scholars argue that these references are evidence of tension between followers of Jesus and followers of John. Mark Goodacre answers your questions about the Shroud of Turin. Jeffery Graff: Can the DNA tests on the bones indicate whether he is a Jew or even whether he is of the tribe of Levi? Moss: I'm so glad someone brought up DNA. The DNA tests on the Bulgarian bones yielded only mitochondrial DNA (DNA passed down by the mother), not the more reliable nuclear DNA (the kind of DNA referred to in forensic investigations). In the original study of the Bulgarian relics (of which I was a part) the mitochondrial DNA revealed that the Bulgarian relics were of \"probable Semitic origin.\" Thinking back to my time in the laboratory with the Copenhagen scientists, I recall that the lead investigator estimated that the probability was about 75%. The episode last night stated things a little too sharply when it said that the bones were from a Middle-Eastern man. As for the more specific question about the genetics of Jews and members of the tribe of Levi: Current scientific technology does not reveal this kind of information even if start-up genetic testing companies promise this kind of information. Cyndi Rosenthal: Are there any other historical references of John the Baptist outside of the Bible? Daniel José Camacho: Any extra-biblical sources that shed light on historical figure of John the Baptist? Moss: (These questions are on a similar topic, so I've chosen to answer them together.) Actually there is external attestation for the life and importance of John the Baptist. This is important because it's fairly rare to find this kind of evidence for ancient figures outside the writings of their followers. The Jewish historian Josephus mentions John the Baptist in his book, \"The Jewish Antiquities.\" Josephus describes John as a \"good man\" who possessed \"virtue\" and had \"great influence\" over the people. According to Josephus, Herod put John the Baptist to death because he was afraid that he might raise a rebellion. This gives us another -- arguably more historical -- perspective on why John was executed and provides further evidence about just how important John was in his own day. Watch the latest full episode anytime on CNNgo . Daniel José Camacho: Wait, how did Jesus get \"Our Father\" prayer from Johnny B??? Didn't catch that. Moss: I'm also really glad someone brought this up, because I wondered about it too. In the Gospel of Luke, one of Jesus' disciples says, \"Lord, teach us to pray, as John taught his disciples.\" It's an interesting request that tells us something about John the Baptist's ministry and the demanding characters of Jesus' disciples! In Luke, Jesus responds to this request by teaching them the Lord's Prayer. We don't know that the Lord's Prayer came from John, and personally I don't think it did; I think this is just how Luke shaped his version of events. But if you were just reading Luke you could come to that conclusion.\n",
            "   3  (CNN)Ferguson is crumbling. The cowardly and reprehensible shooting Wednesday night of two police officers came in a tumultuous seven days for the Missouri town, which had already seen Ferguson Police Chief Thomas Jackson announce his resignation after a damning Justice Department report on its police department. The report, which was ordered in the wake of the killing of Michael Brown last year, highlighted a predatory policing problem and a department that was biased, prejudiced and that has regularly targeted, arrested and fined African-Americans. Residents understandably want justice. But what's worse in all this is that Ferguson is illustrative of a broader problem across the country as increasingly militarized majority-white police departments demonstrate consistent racial bias toward majority-black communities. It's a combustible mix. In three-quarters of all U.S. cities with populations 50,000 or more, the police presence is \"disproportionately white relative to the local population,\" according to The Washington Post. And tensions are being exacerbated by the use among police departments of military weapons, and stipulations that these former war zone weapons must be used within a year of acquisition. All this suggests a need for a completely new mindset on how we try to understand and implement policing practices across America.  Indeed, a wholesale review of policies and approaches to law enforcement is needed, something that will likely necessitate drastic reforms in some departments so they can better represent, integrate, problem solve and liaise with the communities they are serving. First and foremost, our police departments must better reflect the diverse demographics of our increasingly diverse nation, whether it be race, creed, sexual orientation and more. America is changing fast, but police departments aren't keeping up. Training and recruitment of minorities is critical, yet far more needs to be taking place. With this in mind, amplifying community policing models that work and scaling them up immediately is essential if we are to stem the growing and sometimes overwhelming tide of frustration, anger and cynicism welling up among young African, Asian and Hispanic Americans. Second, we must radically rethink the trend toward the indiscriminate procurement and use of surplus military grade weaponry, which under the Department of Defense's 1033 program is flowing from battlefields to our local police forces.  When the weapons of war come home from Iraq and Afghanistan to help police America's cities and towns, then you know something has gone terribly wrong with this country. Mine-resistant ambush protected vehicles, tanks, drones, grenades and assault weapons should not replace the community policing of our Main Streets. Ferguson is an excellent example of how the deployment of heavy weaponry inflames rather than de-escalates a crisis situation. Having military equipment on our streets does not make citizens feel safer, which is why the Stop Militarizing Law Enforcement Act was recently reintroduced in Congress. We do not need our officers looking like \"Robocop\" when they patrol our streets. It's that simple. Yet until such a bill is passed, war weapons will continue to flood our streets; Congress must act to stem this tide. We understand that times have changed and that new security threats require new solutions and procedures.  But it doesn't justify the 150 raids per day by special weapons and tactics units for incidents that can be as benign as a Department of Education warrant. This kind of aggressive approach doesn't engender the kind of engagement necessary for identifying real risks lurking in a community.  In fact, the opposite happens.  Intelligence opportunities are dead on arrival, and potential allies who would otherwise be ready to help shut down immediately. Aggressive military-type action is quickly turning Americans against fellow citizens who they are ostensibly there to serve and protect. We therefore trust that President Barack Obama and Attorney General Eric Holder will take decisive action, working hand in hand with police departments all across this country. Yes, the White House's task force on police militarization was a start, but more concrete measures are needed if we want to reverse the rising anger in Ferguson and elsewhere. The time for a change is now. If we don't press our police departments to reflect the makeup and needs of our communities, then towns like Ferguson will unravel further.\n",
            "   4  Coral Gables, Florida (CNN)Former President Bill Clinton on Saturday defended his family foundation's practice of taking money from foreign countries, arguing that while he doesn't agree with all of the policies of countries that contributed, he feels the foundation has \"done a lot more good than harm.\" The Clinton Foundation admitted last month that a 2010 donation from the Algerian government was not properly approved under the guidelines the Obama administration put in place with the foundation when Hillary Clinton became secretary of state in 2009. Bill Clinton defended the donations as something that went to worthwhile projects. \"The UAE [United Arab Emirates] gave us money. Do we agree with everything they do? No. But they are helping us fight ISIS and they built a great university with NYU open to people around the world,\" Clinton said at a foundation event in Florida. \"Do I agree with all the foreign policy of Saudi Arabia? No.\" Saudi Arabia, the United Arab Emirates and Oman are among the countries that donated to the Clinton Foundation. Clinton continued: \"You've got to decide when you do this work, whether it will do more good than harm if someone helps you from another country. ... I believe we have done a lot more good than harm.\" The story became a controversy for the Clintons, one where even some Democrats questioned the practice. Hillary Clinton is the party's presidential frontrunner in 2016 and is expected to announce her presidential aspirations next month. Republicans jumped on board the story, too, using it to question the Clintons' ethics and whether, as president, she would give preferential treatment to countries that have donated to the foundation. Democrats that did publicly defend the Clintons noted that the foundation disclosed all of their donation on their website. On Saturday, Clinton did the same. \"My theory about all this is disclose everything. And then let people make their judgments,\" Clinton said. \"I'm going to tell you who gave us money and you can make your own decisions.\" Clinton concluded his defense of the foundation, stating that he thinks organizations should \"bring people together across great divides, around things that they can agree on and find something to do to make peoples lives better.\" The Clinton Foundation was founded by Bill Clinton after he left the presidency in 2001. To date, the foundation has raised over $2 billion that goes toward a wide variety of projects, including health and wellness, economic development and leveling the playing field for women and girls. Many of their projects focus on international issues, such as rebuilding Haiti after the 2010 earthquake and providing access to low-cost HIV and AIDS treatment. Those were primarily the projects backed by foreign countries. The former president's comments were the only moment any Clinton mentioned the foreign fundraising controversy. Neither Hillary or Chelsea Clinton mentioned the issue. At no point Saturday did any of the Clintons address the fact that Hillary Clinton exclusively used a private email account for the four years she served as America's top diplomat, a practice that skirted legal standards in place. The former secretary of state spent 18 minutes on stage Saturday and didn't mention question about her email, instead focusing on the foundation and a new report they will put out on women and girls participation. That didn't bother the over 1,000 overachieving millennials at the Clinton Global Initiative University meeting who couldn't have cared less about the swirling email and fundraising controversies that have defined the Clintons the last few weeks. Saturday's event is the university-focused branch of the Clinton Foundation. It brings philanthropic minded students from around the world together to talk about their projects and pitch the foundation for funding. This year brought together students looking to do a wide variety of things, from increasing women's inclusion in science and math fields to harvesting potable water from fog. In total, the foundation will hand out $900,000 to the different students. The general sense among the event attendees was: \"What controversies?\" \"I am here for me. I am here for learning, exploring, meeting new people and expanding my knowledge about nonprofit management and social change,\" said Armel Arnaud Nibasumba, a Middlebury College student born and raised in Burundi. \"I don't really care if they address those political issues that are going on.\" A few students, including Victoria Arild from Menlo College, said they hadn't heard of the controversies. \"I am here because my college, I had the privilege of them funding me,\" she said, before shrugging off questions about the email issues.\n",
            "   5  (CNN)If you haven't yet been asked by friends or co-workers to Meerkat, chances are you will soon. The livestreaming app took Austin by storm last week, with media outlets -- including CNNMoney -- calling it \"the new SXSW sweetheart,\" \"the coolest cat\" and the festival's \"big star.\" The San Francisco-based startup launched February 27 and has been adding users rapidly ever since. Even Jimmy Fallon is streaming his life using Meerkat. As part of our Instant Startups series (see the videos above), CNNMoney correspondent Laurie Segall reached out to Meerkat founder Ben Rubin with some questions. And he agreed to answer them -- on Meerkat, of course. What questions do you have about the app, SXSW or entrepreneurship? Do you dream of launching your own startup? Now is your chance to ask the experts how. Leave your questions in the comments below or ask them directly on Meerkat or Twitter at 2 p.m. ET Wednesday. What: Live chat with Meerkat's Rubin . When: Wednesday, from 2 to 2:45 p.m. ET . Where: On Meerkat and Twitter . Follow on Meerkat: @benrbn and @lauriesegallcnn . Follow on Twitter: @CNNTech . Hashtag: #CNNInstantStartups . Hope to see you there!\n",
            "   6  (CNN)Diplomacy can be dangerous. U.S. diplomats have come under attack in various places in the last few decades. Here's a look at U.S. diplomats who have been killed in the line of duty. The first U.S. ambassador assassinated while in office was John Gordon Mein, the U.S. ambassador to Guatemala. According to a telegram from the embassy in Guatemala City, a young man dressed in fatigues and carrying a sub-machine gun on  August 28, 1968, ordered Mein's vehicle to stop and for the ambassador to get out. He did, then ran -- prompting a cry of \"Shoot him, kill him.\" Mein was shot and fell to the ground about 12 yards behind his limousine. Cleo Noel Jr., the U.S. ambassador to Sudan, was nearing the end of a March 1973 reception in the Saudi Embassy in Khartoum when terrorists stormed in. The gunmen took Noel and another American, as well as diplomats from Saudi Arabia, Belgium and Jordan, according to a U.S. intelligence memo. The captors' demand: Free various people, mostly Palestinian guerillas, then imprisoned in Jordan, Israel and the United States. This spurred negotiations that didn't go anywhere, ending instead with the killing of Noel, fellow U.S. diplomat George Curtis Moore and Belgium's Charge d'Affaires. U.S. authorities say the assailants belonged to the Palestinian terrorist movement known as \"Black September,\" claiming that Palestinian leader Yasser Arafat signed off on the attack. Ambassador Rodger Davies, who had been in Cyprus for less than months, hunkered down in a hallway on August 19, 1974, hoping he was safe from those involved in a nearby demonstration. Instead, a bullet penetrated the embassy compound and struck his heart, killing him instantly. Antoinette Varnava, a 31-year-old local who was part of the small embassy staff for about a decade, also died in the violence. U.S. ambassador to Lebanon Francis Meloy, his economic counselor Robert O. Waring and their Lebanese driver disappeared in June 1976 as they crossed the Green Line, the division between Beirut's Christian and Muslim sectors. Their bullet-riddled bodies were found a short time later in mainly Muslim west Beirut, which was then controlled by PLO Chairman Yasser Arafat's guerrillas. Two former Muslim guerillas were convicted in the kidnapping and killings, only to be freed in 1996. About eight months after President Jimmy Carter appointed him as U.S. ambassador to Afghanistan, Adolph Dubs was taken from his car while heading home from the embassy. His captors took the Foreign Service veteran to the Hotel Kabul, where Dubs died in a shootout between captors and Afghan police -- a violent death that, whomever fired the fatal bullets, the U.S. State Department considers an assassination. A former eight-story hotel facing the sea transformed into America's embassy in Beirut turned into a war zone in April 1983, when a truck loaded with explosives was rammed into its entrance. The result was horrific. Offices were pancaked on top of each other, the elevator shaft and stairwell destroyed, the cafeteria full of bodies and rubble, recalled the then U.S. ambassador Robert Dillon, who himself was dug out of the rubble. While 44 people inside the embassy survived the blast, 17 Americans, 25 Foreign Service nationals, 10 contract workers and 10 visa applicants and passerby did not. In 2008, Dillon said the attack was believed to have been carried out by a family \"under the direction of members of Iran's Revolutionary Guards.\" On August 7, 1998 -- around the exact same time a bomb went off at the U.S. Embassy in Dar es Salaam, Tanzania, killing 11 -- a huge explosion tore through the U.S. Embassy in Nairobi, Kenya. The American Foreign Service Association (AFSA) lists eight people as having died in the attack on its memorial remembering Americans who died while serving the U.S. government abroad in a foreign affairs capacity.  Twelve Americans total were killed. Those are both jarring numbers, but they're still a fraction of the more than 200 people total killed in the attack, in addition to more than 4,000 wounded. In May 2001, a U.S. jury found four purported al Qaeda members guilty on all charges stemming from the embassy bombings in Kenya and Tanzania. As his wife of 34 years looked on, Laurence Foley was shot dead outside his home in Amman, Jordan, by a lone gunman in December 2002. A public servant for close to 40 years, the Boston-born Foley was serving as executive officer of the USAID mission in Amman at the time. U.S. officials were quick to label Foley's killing a murder, with the head of the AFSA calling it a \"brutal terrorist attack.\" They later implicated Iraqi-based terrorist leader Abu Musab al-Zarqawi for providing \"financial and other support to the terrorists who assassinated\" Foley. Exactly 11 years after the September 11 attacks in New York, Washington and rural Pennsylvania, terrorists struck at Americans again -- this time some 5,000 miles away in the Libyan city of Benghazi. That's where Ambassador Christopher Stevens was when mortar and rocket fire struck a U.S. diplomatic annex there. Stevens didn't survive, nor did State Department computer expert Sean Smith or Tyrone Woods and Glen Doherty, two former U.S. Navy SEALs then acting as security contractors. The attack was first portrayed as violence by an angry mob responding to a video made in the U.S. that mocked Islam and the Prophet Mohammed. But officials later determined that it was a terrorist attack. Anne Smedinghoff, a 25-year-old public diplomacy officer at the U.S. Embassy in Kabul, was delivering books to a school in southern Afghanistan when a suicide bomber smashed into her convoy. She died in that April 2013 attack, as did four others. \"We thought she was relatively safe in the embassy compound,\" her father Tom Smedinghoff told CNN. \"But as it turned out, Anne really wanted to do a lot more.\"\n",
            "   7  (CNN)Debra Milke spent 22 years on death row, convicted of conspiring with two other men to kill her son allegedly for an insurance payout. On Monday, a judge ruled that the Arizona woman is innocent and dismissed all charges against her. This makes Milke only the second woman exonerated from death row in the United States. More importantly, the judge's decision finally clears Milke after years of legal back-and-forth in a case where she steadfastly maintained her innocence. Key to the case's dismissal was prosecutorial misconduct, mainly that of a detective, Armando Saldate, who said Milke confessed to the crime to him -- even though there was no witness or recording. Prosecutors withheld from the jury Saldate's personnel record which showed instances of misconduct in other cases, including lying under oath. The two men with whom Milke was accused of conspiring were tried separately and are still on death row. A day after seeing Santa Claus at a mall on December 1, 1989, young Christopher Milke asked his mother if he could go again. Milke's roomate, James Styers, took the boy; then called Milke saying Christopher had disappeared. Instead, Styers and a friend drove the boy out of town to a secluded ravine where Styers shot Christopher three times in the head, prosecutors said. Styers and the friend were convicted of murder and sentenced to death. Milke was implicated based on alleged testimony from Styer's friend, Roger Scott. The detective, Saldate, said Scott told him that Milke was involved in a plot to kill her son. And during her trial, prosecutors floated a likely motive: A $5,000 life insurance policy she had taken out on the child. But neither Scott nor Styers testified to a plot in court. No other witnesses or direct evidence linked Milke to the crime other than Saldate's testimony. Saldate further said that Milke confessed to her role in the murder plot during interrogation and said it was a \"bad judgment call.\" There was no recording of the interrogation, no one else was in the room or watching from a two-way mirror, and Saldate said he threw away his notes shortly after completing his report. Milke offered a vastly different view of the interrogation and denied that she had confessed to any role. The trial became a he-said/she-said contest between the two. Ultimately, the jury believed the detective and convicted Milke of murder. What prosecutors didn't tell the court was the detective's long history of lying under oath and misconduct. Saldate had been suspended five days for taking \"liberties\" with a female motorist and lying about it to his supervisors. Four confessions or indictments had been tossed out because Saldate had lied under oath. Judges suppressed or vacated four other confessions because Saldate had violated a person's constitutional rights. In 2013, after more than 20 years in jail, an appeals court overturned Milke's conviction. \"The Constitution requires a fair trial,\" Chief Judge Alex Kozinski of the federal 9th Circuit Court of Appeals wrote. \"This never happened in Milke's case.\" \"The state knew of the evidence in the personnel file and had an obligation to produce the documents,\" Kozinski said. \"... There can be no doubt that the state failed in its constitutional obligation.\" Milke was released on bail, and the court said she couldn't be tried again. The state appealed the decision to the Arizona Supreme Court. Last week, the Arizona Supreme Court refused to hear the appeal. And on Monday, all charges against Milke were finally dropped. The ankle bracelet she had been wearing while on bail was removed. And Milke left the court room, sobbing in relief. The case is now closed. Debra Milke is finally a free woman.\n",
            "   8  (CNN)A British military health care worker in Sierra Leone has tested positive for Ebola, a UK health agency said. Medical experts are assessing what to do next, including whether or not the evacuate the infected individual to the United Kingdom for treatment, according to a Public Health England spokesperson. An Ebola outbreak has devastated parts of West Africa, with Sierra Leone, Guinea and Liberia being the hardest hit nations. The vast majority of the more than 24,000 confirmed, reportable and suspected cases, as well as the nearly 10,000 reported deaths, have been in those three countries, the World Health Organization reports. In some cases, citizens of other nations have come down with the deadly disease while working there -- as, apparently, is true for the UK military heath care worker whose diagnosis was announced Wednesday. Authorities are investigating how this person was exposed to the virus and tracing individuals in recent contact with the diagnosed worker, said the Public Health England spokesperson. \"Any individuals identified as having had close contact will be assessed and a clinical decision made regarding bringing them to the UK,\" the spokesperson said. Pauline Cafferkey, the first person diagnosed with Ebola in the United Kingdom, was discharged from London's Royal Free Hospital in January after battling the virus. She is a public health nurse in Scotland's South Lanarkshire area who was part of a 30-strong team of medical volunteers deployed to West Africa by the UK government last month in a joint endeavor with Save the Children, according to British media outlets.\n",
            "   9  Baghdad, Iraq (CNN)Iraqi forces appeared Wednesday to be making progress on the third day of a major offensive intended to push ISIS fighters out of the city of Tikrit. The Iraqi soldiers are reportedly approaching the city from five directions, as they seek to prevent ISIS militants from either escaping from Tikrit or sending in reinforcements to bolster its defense. The operation is part of a wide-scale offensive to retake Tikrit and Salahuddin province ordered by Iraqi Prime Minister Haider al-Abadi on Sunday. It has highlighted the role played by neighboring Iran in the fight against ISIS, at a time when the United States and five other world powers are negotiating with Tehran on a controversial deal to curb its nuclear program. The semiofficial Iranian FARS news agency reports that Qassim Sulaimani, the commander of the elite Iranian Al-Quds Brigade, is helping oversee the operation to retake Tikrit. Iran has provided advisers, weapons and ammunition to the Iraqi government. According to the Pentagon, there may be Iranians operating heavy artillery and rocket launchers as well. Despite the reported Iranian assistance, the Iraqi forces face no easy task as they seek to advance on Tikrit. Besides the direct threat posed by ISIS fighters, they must also avoid the large numbers of homemade bombs that litter the approaches to the city. Iraq has not asked the U.S.-led coalition against ISIS to provide air cover for the operation to retake Tikrit. ISIS released a number of propaganda images Wednesday, showing several vehicles and dozens of ISIS militants with their weapons, which it said were part of a military reinforcement of Salahuddin province. CNN cannot independently confirm the authenticity of the photographs. How the U.S. and Iran found common interests . Why battle for Tikrit will defeat ISIS . Tikrit fell to ISIS in June of 2014, after the group's capture of Iraq's second-largest city, Mosul. Tikrit is best known to Westerners as the birthplace of former Iraqi dictator Saddam Hussein. ISIS, the radical Sunni militant group, has been on a murderous campaign to establish a caliphate across swaths of Iraq and Syria. On Monday, Iraqi forces approached Tikrit from several fronts, Iraqiya TV reported, engaging with ISIS north of the city at al-Alam and south of the city at al-Dour. The element of surprise probably was not a factor, as reports of Iraqi troops amassing near Tikrit were widely shared. What awaits the Iraqi army is most likely a long, hard slog and not a quick rout. Tikrit is a big city, and the army and its associated militias have had problems recapturing much smaller towns from ISIS. There have been several failed attempts to recapture Tikrit since the second half of 2014. While Iraqi forces have gained some territory in the area, it has generally been under ISIS control for the last eight months or so. Prime Minister al-Abadi, who is also commander in chief of the armed forces, said on Twitter that he would \"oversee the operation to liberate Tikrit\" from ISIS. The joint Iraqi forces fighting to retake Tikrit include Iraqi troops, members of the Shia al-Hashed al-Shaabi militia, members of the Sunni Sons of Salahuddin brigades and other Sunni tribal fighters. The offensive involves around 30,000 fighters in all. Rights group Human Rights Watch on Wednesday urged government forces to protect civilians from revenge attacks by pro-government militias as they fight to retake Tikrit. The rights group said it had documented \"numerous atrocities against Sunni civilians by pro-government militias and security forces\" after they recaptured other towns. Human Rights Watch also warned that ISIS could seek to use civilians as human shields. \"All commanders in Tikrit need to make sure that their forces protect civilians and allow them to flee the combat zone,\" said Joe Stork, the group's deputy Middle East and North Africa director. \"Past fighting raises grave concerns that Tikrit's civilians are at serious risk from both ISIS and government forces, and both sides need to protect civilians from more sectarian slaughter.\" CNN's Ben Wedeman reported from Baghdad and Laura Smith-Spark wrote in London. CNN's Mariano Castillo and Mohammed Tawfeeq contributed to this report.\n",
            "  10  (CNN)The gift of an inquiring mind can be both exhilarating and torturous. My job is to ask tough questions, but when it comes to faith, God, and religion, the more questions I ask in my quest for truth and understanding, the more complex the answers become. I was a bit of a rebellious child. My mom might tell you differently, but I never saw that as a bad trait. I felt that if I questioned authority, fought for the underdog, battled for the things that people told me were impossible, I would be different. Change the world maybe. That same rebellious spirit also led to things that definitely were not good for me, like hanging with the wrong crowd and getting into the type of trouble that I would rather not put in print. That's when I \"found God.\" I became a \"born-again\" Christian when I attended a Young Life camp in high school. My home life wasn't exactly going swimmingly, and this group really embraced me. I loved the Christian notion of community, giving back, praying for others and making friends that cared more about doing good than getting drunk, smoking pot and having sex. I opened my arms to Jesus and fully embraced Christian morals and principles. I decided that I was going to be \"that good girl\" and go on to do great things. I started off at Westmont, a beautiful Christian college nestled in the heart of Santa Barbara, California. What a safe place that was. It was also extremely nurturing. The professors dedicated bountiful amounts of time to our individual spiritual development, and regularly prayed with us.  My peer group was all about what ministry you signed up for, not what sorority you were rushing. We lifted each other up, had intimate sunrise Bible studies on the beach and spent hours hanging out with friends, talking about how to lead a godly life. As glorious and fulfilling as all that appeared, two years into college, the world became much larger to me. More complex, diverse, intellectually and spiritually challenging. It became the world of church, religion and faith versus the world of ideas, cultures, and philosophies. I found myself more drawn to Carl Jung than the book of Corinthians. A good friend gave me a book, The Myth of Certainty. It posed these questions: . \"Do you ever feel somewhat schizophrenic about the relationship of your faith to the rest of your life? Do you find yourself compartmentalizing different aspects so that tensions between them are minimized?\" The answer to all of these for me was: yes. I started to read a lot. I wanted to mesh with a myriad of thinkers, and religious scholars. I needed to make a change. I left Westmont after my sophomore year and transferred to USC's School of Journalism. I discovered I had too many questions about faith to pursue a life of ministry, but I felt good about this transition. To me, it made perfect sense, because like ministers, true journalists love people, listen well and want to make a difference within this universe. Meet the friendly atheists next door . The key difference is, in journalism, if we gather the \"facts,\" we can usually find the answers to what we're looking for.  When it comes to God, Jesus and the Holy Spirit, those answers rest in faith. As a journalist, I seek intellectual certainty. When it came to my faith, I felt intellectually embarrassed. There was so much I just couldn't explain. When I started working on a documentary about the growth of atheism, I found myself in a profound place of reflection.  In the days when I thought I was going to pursue a life of ministry, I experienced and felt many things that were unexplainable.  What was that?  God?  A higher power?  Energy?  Or just good karma for trying to lead such a generous and selfless life?  There is no way to know. My stepfather, who grew up in -- but later left -- the Mormon church has a perspective on religion that I find intriguing.  He doesn't believe in a God with a long white beard and flowing robes who sits upon a cloud guiding our daily lives.  That concept is too abstract. But while he may not embrace \"God-liness,\" he does believe in \"Good-liness.\"  God, he told me in one of our many colorful spiritual discussions, is the \"good\" in humankind. He and I definitely agree that the concept of God should not be dismissed as having no meaning. To the contrary, it has a very important meaning, for it refers in symbolic language to the highest dimension of human existence, our spirituality. After years of spiritual reflection and inquiry, I am at a place where I don't want to feel guilty, hypocritical, judgmental, closed-minded or arrogant.  So, where do I stand now -- 30 years after \"finding God,\" questioning my faith, committing sins, seeking hazardous adventure and trying to love life and people to the best of my ability? I am a \"seeker.\" A constant seeker within this world, among people and, of course, for spiritual enlightenment of all kind. Because if I did possess the truth -- the \"final answer\" --  I am convinced I would spend the rest of my years missing out on the enrichment and surprise of seeking it. I guess I just love my exhilarating and torturous life.\n",
            "  11  March 3, 2015 . You may not be familiar with the city of Tikrit, but what's happening there now could be a sign of things to come in the war against ISIS. You may not know the name Steve Fossett, but today marks a significant anniversary of one of the adventurer's records. And you may not know how many times glass can be recycled, but we'll tell you on today's edition of CNN Student News. On this page you will find today's show Transcript and a place for you to request to be on the CNN Student News Roll Call. TRANSCRIPT . Click here to access the transcript of today's CNN Student News program. Please note that there may be a delay between the time when the video is available and when the transcript is published. CNN Student News is created by a team of journalists who consider the Common Core State Standards, national standards in different subject areas, and state standards when producing the show. ROLL CALL . For a chance to be mentioned on the next CNN Student News, comment on the bottom of this page with your school name, mascot, city and state. We will be selecting schools from the comments of the previous show. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call! Thank you for using CNN Student News!\n",
            "  12  (CNN)Incumbent Goodluck Jonathan phoned former military leader Muhammadu Buhari on Tuesday to concede defeat in Nigeria's presidential elections, Buhari's party says. Jonathan acknowledged the phone call and his defeat in a written statement to his countrymen. \"I thank all Nigerians once again for the great opportunity I was given to lead this country and assure you that I will continue to do my best at the helm of national affairs until the end of my tenure,\" he said. The Independent National Electoral Commission is still announcing the final tally in the polls, but early numbers indicate Buhari, now the President-elect, has an overwhelming majority of votes. Buhari ruled Nigeria from late 1983 until August 1985 after ousting his predecessor in a coup. His 20-month rule was known for what he described as a \"war on indiscipline,\" a tough regime that some say was marred by human rights abuses. The 72-year-old  retired major general's experience as a military ruler has variably been viewed as a plus or minus in present-day Nigeria, where the government has been locked in a deadly battle with the militant group Boko Haram. His campaign has focused on security and ending corruption in Nigeria. Read more: Who is Nigeria's Muhammadu Buhari? Violent protests after elections Saturday sparked calls for calm from the two main candidates and a warning by the United States and Britain against political interference. Demonstrators fired gunshots and torched a local electoral office in Nigeria's oil-rich Rivers state on Sunday as they marched to protest the elections amid claims of vote-rigging and voter intimidation. After the protests in Rivers, Buhari's All Progressives Congress demanded the elections there be canceled. \"There's been so much violence in Rivers state that it's just not tenable,\" party spokesman Lai Mohammed said. But the People's Democratic Party disputed the accusation, saying the election was \"credible and the result reflects the overwhelming wish of the people of Rivers state to support President Goodluck Jonathan.\" Both candidates took to social media to call for calm. \"I want to urge all Nigerians to also wait patiently for the Independent National Electoral Commission, INEC, to collate and announce results,\" Jonathan said on his Facebook account. \"Fellow Nigerians, I urge you to exercise patience and vigilance as we wait for all results to be announced,\" Buhari said on Twitter. Jonathan and  Buhari last week issued a pledge reaffirming their commitment to \"free, fair and credible elections\" after their signing of  the Abuja Accord in January. In his statement Tuesday, Jonathan said; \"I promised the country free and fair elections. I have kept my word.\" He advised anyone upset with the results to follow due process and stay away from further violence. \"As I have always affirmed, nobody's ambition is worth the blood of any Nigerian. The unity, stability and progress of our dear country is more important than anything else,\" he said. Read more: Democracy was the real winner . More than 800 people were killed in post-election violence across Nigeria's north in 2011 after charges that those elections were illegitimate. Nigeria's vote had been scheduled for February 14, but on February 7, Nigeria's election commission announced it would be postponed for six weeks because of security concerns, with the military needing more time to secure areas controlled by Boko Haram. The controversial decision was unpopular among many Nigerians and led to widespread protests. Jonathan has been criticized for not doing enough to combat Boko Haram, which is waging a campaign of terror aimed at instituting a stricter version of Sharia law in Nigeria. On Saturday, residents in the northeastern state of Gombe said at least 11 people were killed and two more injured in attacks at polling stations, apparently by Boko Haram extremists. CNN's Christian Purefoy reported from Lagos, Nigeria, while CNN's Susannah Cullinane and Stephanie Busari wrote from London.\n",
            "  13  (CNN)Wildlife services in California are being pushed to their limits this year. Since January 2015, every month has set a record in sea lion \"strandings,\" mostly sea lion pups, according to the National Oceanic and Atmospheric Administration. \"There has been an unusually high number of sea lions stranded since January,\" said Justin Greenman, assistant stranding coordinator for NOAA on the West Coast. \"Stranding does happen, but just to give you perspective, 1,800 [sea lion] pups have been responded to this year alone. We responded to 1,600 strandings total during the entire year in 2013,\" he said. Stranding is the official term to describe marine life that \"swim or float into shore and become beached or stuck,\" according to NOAA. Strandings are taking a toll on the resources available in coastal counties from San Diego to Santa Barbara. Local care facilities have taken in more stranded sea lions this year than 2004-12 combined, and it is only mid-March. Greenman said he expects the problem to continue beyond April, when weaning normally occurs, when the pups are 10 or 11 months old. Dave Koontz, director of communications for SeaWorld San Diego, said SeaWorld has rescued nearly 500 sea lions this year. \"This is a new record for Sea World,\" Koontz said. \"In 1983 we rescued 474.\" Some of the sea lions responded to have had to be euthanized. \"They [sea lion pups] have to be able to eat and fish on their own before they can be released back into the wild, and a lot of these pups haven't even been weaned,\" Greenman said. Greenman said California has had warmer weather than usual this year, and, while NOAA is still conducting studies on the Channel Islands to get a more proven explanation, warmer water drives the food source farther out or deeper into the ocean, where the colder water is. When food is farther away, the mothers are away from the pup too long in search of food, and return with little food or too few nutrients for a growing sea lion. \"We have been seeing emaciated or dehydrated sea lions show up on beaches,\" Greenman said. However, he said, the species has made a comeback since the Marine Mammal Protection Act of 1972. California's sea lion population has grown to 300,000 from an estimated population of 10,000 in the 1950s, according to the Washington Department of Fish & Wildlife. People who observe stranded sea lions are advised not to touch them or attempt to rescue them, because it can be dangerous and it is illegal. Instead, call any of the rescue agencies listed on NOAA West Coast Region's website. If the animal has died, the local dead animal pickup service should be alerted.\n",
            "  14  (CNN)Germanwings co-pilot Andreas Lubitz was hiding an illness from his employers and had been declared \"unfit to work\" by a doctor, according to German authorities investigating what could have prompted the seemingly competent and stable pilot to steer his jetliner into a French mountain. Investigators found a letter in the waste bin of his Dusseldorf, Germany, apartment saying that Lubitz, 27, wasn't fit to do his job, city prosecutor Christoph Kumpa said Friday. The note, Kumpa said, had been \"slashed.\" Just what was ailing Lubitz hasn't been revealed. The New York Times and the Wall Street Journal, citing unnamed sources, reported Friday that Lubitz suffered from mental illness and kept his diagnosis concealed from his employer. A Dusseldorf clinic said he'd gone there twice, most recently 17 days ago, \"concerning a diagnosis.\" But the University Clinic said it had not treated Lubitz for depression. German investigators said they still have interviews and other work to do before they'll be able to reveal just what they found in the records in Lubitz's apartment in a quiet, suburban neighborhood. They found no goodbye note or confession, authorities said. But the fact that investigators found \"ripped, recent medical leave notes, including for the day of the offense, leads to the preliminary conclusion that the deceased kept his illness secret from his employer and his professional environment,\" prosecutors said. Authorities left Lubitz's apartment  Friday night with boxes of papers and evidence folders after spending about 90 minutes inside. According to authorities in Germany and France, Lubitz was a co-pilot on Germanwings Flight 9525 between Barcelona, Spain, and Dusseldorf on Tuesday when he apparently locked the captain out of the cockpit, then activated a control causing the plane to descend toward rugged terrain. Germanwings said the plane dropped for about eight minutes from its cruising altitude of 38,000 feet before crashing. The only sounds, authorities said, were those of pounding on the cockpit door, Lubitz's steady breathing and, eventually, screaming passengers. An 8-minute descent to death . Lubitz and 149 other people on board the plane died in an instant, authorities say. Mother, daughter among 3 American victims . What could have prompted Lubitz to deliberately destroy the aircraft, killing everyone on board, remained the focus of investigators in Germany. Officials said Lubitz was not known to be on any terrorism list, and his religion was not immediately known. He had passed medical and psychological testing when he was hired in 2013, said Carsten Spohr, CEO of Lufthansa, which owns Germanwings. While the ailment Lubitz had sought treatment for hasn't been revealed, that he was declared unfit for work is an important detail, aviation analysts say. Pilots are required to maintain their fitness to fly and must tell their airline if they're found unfit, CNN aviation analyst David Soucie said. Reuters reported that a German newspaper said Lubitz had been treated for depression about six years ago. Citing internal documents forwarded by Lufthansa to German authorities, Bild reported that Lubitz had suffered a \"serious depressive episode\" around the time he took a break from his pilot training in 2009, Reuters reported. The Bild report said he then spent about 18 months getting psychiatric treatment.  Lufthansa officials and German prosecutors declined to comment on the Bild story, Reuters said. Who was co-pilot Andreas Lubitz? Although authorities have recovered the cockpit voice recorder, the flight data recorder remains missing. It could shed crucial details about what happened inside the cockpit, authorities say. Rescuers have found bodies at the rugged crash site, but few of them are intact, Yves Naffrechoux, captain of rescue operations at Seyne-les-Alpes, told CNN. Dangerous and windy condition at the remote site, which covers more than a square mile, are hampering efforts to recover bodies and evidence, he said. Officials with experience traversing the French Alps are helping technicians who don't have alpine skills, he said. \"Since they don't know the mountains, you need to provide them with equipment, you need to hold them with rope, give them crampons so they can work well and as precisely as possible, so that no evidence, no body part could escape their vigilance,\" Naffrechoux said. Workers are now looking into the possibility of building a road to the site, Naffrechoux said. Recovery teams have made good progress, a French senior paramilitary police official told CNN. Gendarmerie Lt. Col. Jean-Paul Bloy, who is coordinating the helicopter operations for the crash site, said there will be two aircraft deployed over the scene Saturday. There were five on Friday, Bloy said. As that difficult work continued, relatives and friends of the victims traveled on Lufthansa flights to an area near the site where their loved ones perished. They held prayers in Le Vernet, near Seyne-les-Alpes, a village serving as a staging post for the recovery operation. Flowers and pictures sat on the ground, candles flickering in the cold air. Germanwings said it was setting up a family assistance center in Marseille, France, with family briefings to start Saturday. Another flight carrying victims' relatives was due to arrive in Marseille from Barcelona on Friday. \"Our focus in these darkest hours is to provide psychological assistance to the families and friends of the victims,\" said Thomas Winkelmann, a spokesman for the Germanwings executive board. It could be weeks before all the bodies are recovered, identified and released to the families, authorities said. Meanwhile, the European Aviation Safety Agency issued a temporary recommendation that cockpits always be staffed by at least two crew members. \"While we are still mourning the victims, all our efforts focus on improving the safety and security of passengers and crews,\" the agency's director, Patrick Ky, said in a statement. Lufthansa and other German airlines have already adopted the rule, the airline said. An official with the German Aviation Association told CNN that it was only a matter of hours, or a day at most, for this rule to be implemented across all big German airlines. A pilot aboard a Germanwings flight Friday morning spoke out at the beginning of the trip to \"reassure passengers that there will be two people present in the cockpit at all times.\" Lufthansa will now keep two crew members in cockpits . CNN's Catherine E. Shoichet, Greg Botelho, Claudia Rebaza, Frederik Pleitgen, Nic Robertson, Margot Haddad, Stephanie Halasz, Khushbu Shah, Bharati Naik, Ingrid Formanek, Sandrine Amiel, Rosie Tomkins, Will Ripley and Anna Maja Rappard contributed to this report.\n",
            "  15  (CNN)I was deeply moved watching Julianne Moore win the Oscar for \"Still Alice,\" a movie I was proud and privileged to be an executive producer on. Julianne gives a harrowing performance as a brilliant 50-year-old college professor who loses her brain and herself to early-onset Alzheimer's disease. This is a huge moment for Julianne -- and a huge moment for all of us who have been trying to focus public attention on this staggering disease. Witnessing Alzheimer's progress on the big screen is as terrifying as it is in real life. I know, because I'm a child of Alzheimer's. The mind of my father, Sargent Shriver, had always been a finely tuned instrument that left people in awe and inspired. But my family and I watched Alzheimer's erase that brain -- slowly, inexorably, completely. It was terrifying, too, because back then, the disease was surrounded by shame and silence. Alzheimer's still carries a stigma of the unknown -- even though today more than 5 million Americans have it. That's right. Every 67 seconds, another one of us develops Alzheimer's. Women in their 60s are about twice as likely to develop Alzheimer's as breast cancer. With 10,000 baby boomers turning 65 every day, there will be 13.5 million of us with Alzheimer's by 2050. And many people don't understand that Alzheimer's isn't a natural part of aging. Alzheimer's is a disease that kills. The truth is, we're right in the middle of an epidemic, but we as a nation are in denial. An Oscar for \"Still Alice\" is shining the brightest light yet on Alzheimer's, but light isn't enough anymore. Attention isn't enough. It's time to get serious. Alzheimer's is exerting a powerful impact on American families -- on our health, our finances, and our futures. And women are disproportionately affected. Why women? Back in 2010, when we published \"The Shriver Report: A Woman's Nation Takes on Alzheimer's,\" we reported that women were more than half the individuals diagnosed with Alzheimer's and nearly two-thirds of the unpaid caregivers of those who had it. Now those numbers are far worse. Today nearly two-thirds of those with Alzheimer's are women -- that's more than 3.2 million women. And more than 60% of caregivers for people with Alzheimer's and dementia are women, with many having to reduce their own workload or even drop out of the workforce altogether to care for loved ones. Opinion: Why 'Still Alice' is about you . Women are the epicenter of this crisis, which is why I believe they also have to be the solution. So, in partnership with the Alzheimer's Association and so many inspiring women already working on the front lines to fight this disease, we have launched the Wipe Out Alzheimer's Challenge, a multipronged campaign powered by women's brains. Our mission is to enlist women of all ages to get educated, engaged and empowered to instigate change.   Women around the country will go out and raise the alarm, raise awareness, raise the stakes and raise millions of dollars to fund serious research into women's brains. And there's so much research to do and so many questions to answer. Why is the incidence of Alzheimer's higher for women? Nobody knows. And why is it that women in their 60s are so much more likely to get Alzheimer's than breast cancer? Nobody knows. What's the exact role of estrogen? We don't know. Is there an Alzheimer's connection with depression or with diabetes? What about genetics? What can be done during the 20 or so years when the disease develops, before a woman even becomes symptomatic? What's the impact of diet, stress level, exercise, sleep and cardiovascular condition? It's time to find out. We have to fund this research, because for some reason it's not a priority for the government. In 2015, Washington will spend an estimated $6 billion on cancer research and $3 billion on HIV/AIDS research, but only $586 million on Alzheimer's. Yet, as the Alzheimer's Association tells us, \"the costs to all payers for the care of people living with Alzheimer's disease and other dementias will total an estimated $226 billion, with Medicare and Medicaid paying 68 percent of the costs.\" I don't get it, but I'm not going to wait anymore. So Wipe Out Alzheimer's is stepping in. We're asking women to put together their own \"brain trusts\" in their communities -- groups that will go out and do some muscular fund-raising. But equally important, these brain trusts will gather to discuss and disseminate information about what the disease is and isn't. What are the warning signs we should look for in ourselves and our parents? What's the difference between normal forgetfulness, dementia and Alzheimer's disease? Can brain games or meditation slow cognitive decline? Do dietary supplements help? Local brain trust groups will also learn about the devastatingly high cost of Alzheimer's -- how neither Medicare nor the Affordable Care Act covers long-term care, and the cost of a semiprivate nursing home room averages more than $80,000 a year. They'll reach out to help and encourage women whose loved ones have Alzheimer's. They will be politically engaged and encourage political candidates who support increased funding for Alzheimer's research. They'll push their own doctors to get better-educated about cognitive health. It's time for the narrative around Alzheimer's to change. I remember when an HIV/AIDS diagnosis was a death sentence. I remember when cancer was a dirty word, and the prognosis was always grim. But AIDS and cancer activists are helping to take these diseases from terrifying to treatable, from hopeless to hopeful. We want to do the same with Alzheimer's. We want to understand it, prevent it, treat it and beat it.  Wipe Out Alzheimer's is creating a global community of women activists, agitators and agents of change to do just that. We used to think that the mysterious condition called Alzheimer's disease happened only to folks in their 80s and 90s. \"Still Alice\" shows us that's just not true. The race for the Oscar may be over, but the race to wipe out Alzheimer's is on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLiP9Q5QWg-A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}